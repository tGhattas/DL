
%This is my super simple Real Analysis Homework template

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage{minted}
\usepackage{graphicx}

\title{Homework 1}
\author{Tamer Ghattas and Daniel Kerbel}
\date\today
%This information doesn't actually show up on your document unless you use the maketitle command below

\begin{document}
\maketitle %This command prints the title based on information entered above

%Section and subsection automatically number unless you put the asterisk next to them.
\section*{Section 2}

\subsection*{Problem 1}

\begin{proof}
Given functions $f, g:\mathcal{F}^m\rightarrow\mathcal{F}^n$ are linear, and $a,\ b\ \mathcal{\in F}^m$ and $c$ scalar.
\begin{itemize}
\item$
f(g(a+b))=_{g\ linear}f(g(a)+g(b))=_{f\ linear}f(g(a))+f(g(b))
$
\item$
f(g(c\cdot a))=_{g\ linear}f(c\cdot g(a))=_{f\ linear}c\cdot f(g(a))
$
\end{itemize}

Therefore the composition of linear functions are linear.
\end{proof}

\begin{proof}
Given functions $f, g:\mathcal{F}^m\rightarrow\mathcal{F}^n$ are affine and $M_{1},\ M_{2}\ \mathcal{\in F}^{m x n}$ and $b_{1}$, $b_{2}\ \mathcal{\in F}^m$ \. s.t for $x \mathcal{\in F}^{n}$ \\
\[
f(x) = M_{1}x +b_{1} \ \text{and}\ g(x) = M_{2}x + b_{2}
\]
then\\
\[ 
f(g(x))=M_{1}g(x)+b_{1}=M_{1}(M_{2}x+b_{2})+b_{2}=
\]
\[ 
{M_{1}M_{2}}x+(M_{1}b_{2}+b_{2})
\]
which can be written as
\[
M_{3}x +b_{3} 
\]
Therefore the composition of affine functions are affine.
\end{proof}


\clearpage %Gives us a page break before the next section. Optional.

\subsection*{Problem 2}

\begin{enumerate}
    \item The given iterative scheme stops when $f\left(x^{n+1}\right)=f\left(x^{n}\right)$ that is on $-\alpha \nabla f\left(x^{n}\right)=0$ and that occurs for $\alpha \neq 0$ only when $$\left(\frac{\partial f}{\partial x_{1}}, \ldots, \frac{\partial f}{\partial x_{n}}\right)=0$$ which means that the iterative scheme stops on a local minimum or maximum.
    
    \item let $x\in \mathcal{R}^{n}$ w.l.o.g be a local minimum, then
    $$(*)\ f(x+dx)-f(x) \geq 0$$
    and
    $$\nabla f(x) = 0$$
    Using the second-order multivariate Taylor theorem, we know that
    $$f(x+d x)=f(x)+\nabla f(x) \cdot d x+d x^{T} \cdot H(x) \cdot d x+O\left(\|d x\|^{3}\right)$$
     Corollary, we have:
    $$f(x+d x)=f(x)+0 \cdot d x+d x^{T} \cdot H(x) \cdot d x+O\left(\|d x\|^{3}\right)=$$
    $$f(x+d x)-f(x)=d x^{T} \cdot H(x) \cdot d x+O\left(\|d x\|^{3}\right)=$$
    by (*) we get:
    $$0\leq d x^{T} \cdot H(x) \cdot d x+O\left(\|d x\|^{3}\right)$$
    We conclude that if the Hessian matrix $H(x)$ is positive semi-definite then x is a local minimum.
    Note that for local maximum the transitions are symmetric and leads to the Hessian being negative definite as a sufficient condition.
\end{enumerate}



\subsection*{Problem 3}

\begin{minted}{python}
class CircularError(Loss):

  def call(self, y_true, y_pred):
    y_pred = tf.convert_to_tensor_v2(y_pred)
    return tf.reduce_mean(1-cos(y_pred - y_true), axis=-1)
\end{minted}

\clearpage
\subsection*{Problem 4}

need to proof that $f(x)=\Sigma_{i} \alpha_{i}\cdot ReLU(w_{i}x+b_{i})$ is dense in [0,1].
Denote $\sigma(x)$ the known Sigmoid function. We observe that for scalar $\alpha$
$$\alpha_{i} ReLU(w_{i}x+b_{i})=\left\{\begin{array}{ll}
0 & \text { if }\ w_{i}x+b_{i}<0 \\
\alpha_{i}(w_{i}x+b_{i}) & \text { if }\ w_{i}x+b_{i} \in[0,1] \\
\end{array}\right.$$
Define $M_{i}$ as the maximum of $|\alpha_{i}(w_{i}x+b_{i})|$ for $x \in [0,1] $, that is, 
$$ R_{i}(x) =_{define}  \alpha_{i} ReLU(w_{i}x+b_{i}) $$
where,
$$|R_{i}(x)|  \leq M_{i}$$
and for the sum over i:
$$|\Sigma_{i} R_{i}(x)|\leq_{triangle\ in-eq}  \Sigma_{i} |R_{i}(x)|  \leq \Sigma_{i}M_{i}=_{define} M$$
thus,
$$R(x) =_{define} \Sigma_{i} \alpha_{i} ReLU(w_{i}x+b_{i}) \in [-M,M] $$
we know from Hornik 91 theorem that any bounded function *NEEDS CHANGE*

\subsection*{Problem 5}
Assuming the activation function is ReLU, then to deal with negative coefficients one can simple add a layer to add over $ReLU(h)+ReLU(-h)$, note that one of them will nullify while the other propagate:

\begin{figure}[h!]
  \caption{Original nodes are gray colored while added ones colored in white.}
  \centering
  \includegraphics[width=1\textwidth]{HW1-ex5--trans}
\end{figure}

\end{document}